
.. _quickstart:

Quickstart
----------

The basic usage of ``dit`` corresponds to creating distributions, modifying
them if need be, and then computing properties of those distributions.
First, we import:

.. ipython::

    In [1]: import dit

Suppose we have a really thick coin, one so thick that there is a reasonable
chance of it landing on its edge. Here is how we might represent the coin in
``dit``.

.. ipython::

    In [2]: d = dit.Distribution(['H', 'T', 'E'], [.4, .4, .2])

    @doctest
    In [3]: print(d)
    Class:          Distribution
    Alphabet:       ('E', 'H', 'T') for all rvs
    Base:           linear
    Outcome Class:  str
    Outcome Length: 1
    RV Names:       None

    x   p(x)
    E   0.2
    H   0.4
    T   0.4

Calculate the probability of :math:`H` and also of the combination:
:math:`H~\mathbf{or}~T`.

.. ipython::

    @doctest float
    In [4]: d['H']
    Out[4]: 0.4

    @doctest float
    In [50]: d.event_probability(['H','T'])
    Out[50]: 0.8

Calculate the Shannon entropy and extropy of the joint distribution.

.. ipython::

    @doctest float
    In [10]: dit.shannon.entropy(d)
    Out[10]: 1.5219280948873621

    @doctest float
    In [11]: dit.other.extropy(d)
    Out[11]: 1.1419011889093373

Create a distribution representing the :math:`\mathbf{xor}` logic function.
Here, we have two inputs, :math:`X` and :math:`Y`, and then an output
:math:`Z = \mathbf{xor}(X,Y)`.

.. ipython::

    In [6]: import dit.example_dists

    In [7]: d = dit.example_dists.Xor()

    In [8]: d.set_rv_names(['X', 'Y', 'Z'])

    @doctest
    In [9]: print(d)
    Class:          Distribution
    Alphabet:       ('0', '1') for all rvs
    Base:           linear
    Outcome Class:  str
    Outcome Length: 3
    RV Names:       ('X', 'Y', 'Z')

    x     p(x)
    000   0.25
    011   0.25
    101   0.25
    110   0.25

Calculate the Shannon mutual informations :math:`\I[X:Z]`, :math:`\I[Y:Z]`, and
:math:`\I[X,Y:Z]`.

.. ipython::

    @doctest float
    In [12]: dit.shannon.mutual_information(d, ['X'], ['Z'])
    Out[12]: 0.0

    @doctest float
    In [13]: dit.shannon.mutual_information(d, ['Y'], ['Z'])
    Out[13]: 0.0

    @doctest float
    In [14]: dit.shannon.mutual_information(d, ['X', 'Y'], ['Z'])
    Out[14]: 1.0

Calculate the marginal distribution :math:`P(X,Z)`.
Then print its probabilities as fractions, showing the mask.

.. ipython::

    In [15]: d2 = d.marginal(['X', 'Z'])

    @doctest
    In [16]: print(d2.to_string(show_mask=True, exact=True))
    Class:          Distribution
    Alphabet:       ('0', '1') for all rvs
    Base:           linear
    Outcome Class:  str
    Outcome Length: 2 (mask: 3)
    RV Names:       ('X', 'Z')

    x     p(x)
    0*0   1/4
    0*1   1/4
    1*0   1/4
    1*1   1/4

Convert the distribution probabilities to log (base 3.5)
probabilities, and access its probability mass function.

.. ipython::

    In [17]: d2.set_base(3.5)

    @doctest float
    In [18]: d2.pmf
    array([-1.10658951, -1.10658951, -1.10658951, -1.10658951])

Draw 5 random samples from this distribution.

.. ipython::
   :doctest:

    @suppress
    In [19]: dit.math.prng.seed(1)

    In [19]: d2.rand(5)
    Out[19]: ['01', '10', '00', '01', '00']

Enjoy!
